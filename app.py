import streamlit as st
from transformers import pipeline
import torch

# Use a cache to load the model only once
@st.cache_resource
def load_model():
    """Loads the text generation model."""
    return pipeline('text-generation', model='distilgpt2')

generator = load_model()

# --- App Title and Description ---
st.title("ðŸ¤– AI Social Media Post Generator")
st.markdown("This app uses the `distilgpt2` model to generate social media posts based on your inputs. Fill in the details below to get started!")

# --- User Inputs ---
with st.form("post_form"):
    topic = st.text_input(
        "**Topic or Idea**", 
        placeholder="e.g., launching a new line of eco-friendly sneakers"
    )
    
    platform = st.selectbox(
        "**Platform**",
        ('Instagram', 'Twitter', 'Facebook', 'LinkedIn')
    )
    
    tone = st.selectbox(
        "**Tone**",
        ('Excited', 'Professional', 'Casual', 'Funny', 'Inspirational')
    )
    
    submitted = st.form_submit_button("âœ¨ Generate Post")

# --- Generation Logic ---
if submitted:
    if not topic:
        st.error("Please enter a topic before generating.")
    else:
        with st.spinner("ðŸ¤– AI is thinking... Please wait."):
            # The one-shot prompt from your notebook code
            prompt = f"""
Generate a social media post by following the example format.

---
*EXAMPLE*
*Platform:* Instagram
*Topic:* A new bookstore cafe is opening.
*Tone:* Cozy and inviting

*Generated Post:*
*Caption:* The day has finally come! â˜•ðŸ“š We're so excited to announce the grand opening of 'The Reading Nook Cafe' this Saturday. Come find your next favorite book and enjoy a warm cup of coffee with us. We can't wait to welcome you to our cozy corner of the world!
*Hashtags:* #BookstoreCafe #GrandOpening #CoffeeAndBooks #NewBeginnings #CozyVibes
---

*YOUR TASK*
*Platform:* {platform}
*Topic:* {topic}
*Tone:* {tone}

*Generated Post:*
*Caption:*"""
            
            try:
                # --- CORRECTED MODEL CALL ---
                # This version tells the model to only return the new text it generates,
                # which is more reliable than splitting the text afterwards.
                generated_outputs = generator(
                    prompt,
                    max_new_tokens=150,      # Generate up to 150 new words
                    return_full_text=False,  # This is the key change!
                    pad_token_id=generator.tokenizer.eos_token_id
                )
                
                # The output is now much cleaner, no splitting needed.
                final_post = generated_outputs[0]['generated_text'].strip()

                st.subheader("âœ… Here's Your Generated Post:")
                st.markdown(f"> {final_post}")

            except Exception as e:
                st.error(f"An error occurred: {e}")
